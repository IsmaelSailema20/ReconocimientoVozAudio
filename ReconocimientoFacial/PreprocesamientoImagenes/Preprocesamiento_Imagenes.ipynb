{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07316a3",
   "metadata": {},
   "source": [
    "# Proyecto Final Inteligencia Artificial\n",
    "## Universidad Técnica Ambato\n",
    "\n",
    "**Integrantes:** Ismael Sailema - William Chimborazo\n",
    "\n",
    "\n",
    "**Curso:** 7mo Software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c714d",
   "metadata": {},
   "source": [
    "# 1. Importaciones necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e466c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38359f94",
   "metadata": {},
   "source": [
    "# 2. Verficación de videos disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6721665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DIAGNÓSTICO DE VIDEOS\n",
      "======================================================================\n",
      "Carpeta encontrada: TrainingData/videos\n",
      "Personas encontradas: 4\n",
      "  - AlisonSalas: 2 videos\n",
      "      Alison_video_1.mp4\n",
      "      Alison_video_2.mp4\n",
      "  - FreddyAlvarez: 3 videos\n",
      "      Freddy_video_01.mp4\n",
      "      Freddy_video_02.mp4\n",
      "  - IsmaelSailema: 3 videos\n",
      "      Ismael_video_01.mp4\n",
      "      Ismael_video_02.mp4\n",
      "  - WilliamChimborazo: 2 videos\n",
      "      William_video_1.mp4\n",
      "      William_video_2.mp4\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Verificar que la carpeta de videos existe y contiene videos\n",
    "video_base = 'TrainingData/videos'\n",
    "video_base_path = Path(video_base)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DIAGNÓSTICO DE VIDEOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if not video_base_path.exists():\n",
    "    print(f\"Carpeta no existe: {video_base}\")\n",
    "else:\n",
    "    print(f\"Carpeta encontrada: {video_base}\")\n",
    "    \n",
    "    # Listar carpetas de personas\n",
    "    person_dirs = sorted([d for d in video_base_path.iterdir() if d.is_dir()])\n",
    "    print(f\"Personas encontradas: {len(person_dirs)}\")\n",
    "    \n",
    "    for person_dir in person_dirs:\n",
    "        videos = list(person_dir.glob('*.mp4')) + list(person_dir.glob('*.avi'))\n",
    "        print(f\"  - {person_dir.name}: {len(videos)} videos\")\n",
    "        for vid in videos[:2]:\n",
    "            print(f\"      {vid.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b620839",
   "metadata": {},
   "source": [
    "# 3. Extracción de Frames\n",
    "\n",
    "    Extrae frames de videos organizados por persona usando OpenCV (cv2)\n",
    "    Guarda los frames en carpetas con nombres de personas\n",
    "    \n",
    "    Args:\n",
    "        video_base_folder: Carpeta raíz con subcarpetas de personas\n",
    "        output_base_folder: Carpeta raíz donde guardar frames\n",
    "        fps: Frames por segundo a extraer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc0f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer frames de videos usando cv2\n",
    "def extract_frames_from_videos(video_base_folder, output_base_folder, fps=2):\n",
    "    from pathlib import Path\n",
    "    \n",
    "    video_base = Path(video_base_folder)\n",
    "    output_base = Path(output_base_folder)\n",
    "    output_base.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Iterar por carpetas de personas\n",
    "    person_dirs = sorted([d for d in video_base.iterdir() if d.is_dir()])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXTRAYENDO FRAMES DE {len(person_dirs)} PERSONA(S)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    total_videos = 0\n",
    "    total_frames = 0\n",
    "    \n",
    "    for person_dir in person_dirs:\n",
    "        person_name = person_dir.name\n",
    "        print(f\"Procesando: {person_name}\")\n",
    "        \n",
    "        # Encontrar videos\n",
    "        video_files = list(person_dir.glob('*.mp4')) + list(person_dir.glob('*.avi')) + list(person_dir.glob('*.mov'))\n",
    "        \n",
    "        if not video_files:\n",
    "            print(f\"  No se encontraron videos\")\n",
    "            continue\n",
    "        \n",
    "        for video_idx, video_file in enumerate(video_files, 1):\n",
    "            # Crear carpeta con nombre de la persona\n",
    "            person_output = output_base / person_name\n",
    "            person_output.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            print(f\"  Video {video_idx}: {video_file.name}\")\n",
    "            \n",
    "            # Abrir video con cv2\n",
    "            cap = cv2.VideoCapture(str(video_file))\n",
    "            \n",
    "            if not cap.isOpened():\n",
    "                print(f\"Error: No se pudo abrir el video\")\n",
    "                continue\n",
    "            \n",
    "            # Obtener FPS del video original\n",
    "            video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            if video_fps == 0:\n",
    "                video_fps = 30  # FPS por defecto si no se puede obtener\n",
    "            \n",
    "            # Calcular cada cuántos frames extraer\n",
    "            frame_interval = int(video_fps / fps)\n",
    "            if frame_interval < 1:\n",
    "                frame_interval = 1\n",
    "            \n",
    "            frame_count = 0\n",
    "            saved_count = 0\n",
    "            \n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Guardar solo cada N frames según el FPS deseado\n",
    "                if frame_count % frame_interval == 0:\n",
    "                    output_filename = f'{person_name}_video{video_idx}_frame_{saved_count:04d}.png'\n",
    "                    output_path = person_output / output_filename\n",
    "                    cv2.imwrite(str(output_path), frame)\n",
    "                    saved_count += 1\n",
    "                \n",
    "                frame_count += 1\n",
    "            \n",
    "            cap.release()\n",
    "            \n",
    "            if saved_count > 0:\n",
    "                print(f\"{saved_count} frames extraídos\")\n",
    "                total_frames += saved_count\n",
    "                total_videos += 1\n",
    "            else:\n",
    "                print(f\"No se extrajeron frames\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RESUMEN DE EXTRACCIÓN:\")\n",
    "    print(f\"  Total de videos procesados: {total_videos}\")\n",
    "    print(f\"  Total de frames extraídos: {total_frames}\")\n",
    "    print(f\"  Método utilizado: OpenCV (cv2)\")\n",
    "    print(f\"{'='*70}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58e7b6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXTRAYENDO FRAMES DE 4 PERSONA(S)\n",
      "======================================================================\n",
      "\n",
      "Procesando: AlisonSalas\n",
      "  Video 1: Alison_video_1.mp4\n",
      "430 frames extraídos\n",
      "  Video 2: Alison_video_2.mp4\n",
      "430 frames extraídos\n",
      "  Video 2: Alison_video_2.mp4\n",
      "120 frames extraídos\n",
      "Procesando: FreddyAlvarez\n",
      "  Video 1: Freddy_video_01.mp4\n",
      "120 frames extraídos\n",
      "Procesando: FreddyAlvarez\n",
      "  Video 1: Freddy_video_01.mp4\n",
      "260 frames extraídos\n",
      "  Video 2: Freddy_video_02.mp4\n",
      "260 frames extraídos\n",
      "  Video 2: Freddy_video_02.mp4\n",
      "104 frames extraídos\n",
      "  Video 3: Freddy_video_03.mp4\n",
      "104 frames extraídos\n",
      "  Video 3: Freddy_video_03.mp4\n",
      "129 frames extraídos\n",
      "Procesando: IsmaelSailema\n",
      "  Video 1: Ismael_video_01.mp4\n",
      "129 frames extraídos\n",
      "Procesando: IsmaelSailema\n",
      "  Video 1: Ismael_video_01.mp4\n",
      "129 frames extraídos\n",
      "  Video 2: Ismael_video_02.mp4\n",
      "129 frames extraídos\n",
      "  Video 2: Ismael_video_02.mp4\n",
      "212 frames extraídos\n",
      "  Video 3: Ismael_video_3.mp4\n",
      "212 frames extraídos\n",
      "  Video 3: Ismael_video_3.mp4\n",
      "285 frames extraídos\n",
      "Procesando: WilliamChimborazo\n",
      "  Video 1: William_video_1.mp4\n",
      "285 frames extraídos\n",
      "Procesando: WilliamChimborazo\n",
      "  Video 1: William_video_1.mp4\n",
      "433 frames extraídos\n",
      "  Video 2: William_video_2.mp4\n",
      "433 frames extraídos\n",
      "  Video 2: William_video_2.mp4\n",
      "146 frames extraídos\n",
      "\n",
      "======================================================================\n",
      "RESUMEN DE EXTRACCIÓN:\n",
      "  Total de videos procesados: 10\n",
      "  Total de frames extraídos: 2248\n",
      "  Método utilizado: OpenCV (cv2)\n",
      "======================================================================\n",
      "\n",
      "146 frames extraídos\n",
      "\n",
      "======================================================================\n",
      "RESUMEN DE EXTRACCIÓN:\n",
      "  Total de videos procesados: 10\n",
      "  Total de frames extraídos: 2248\n",
      "  Método utilizado: OpenCV (cv2)\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extraer frames de todos los videos\n",
    "extract_frames_from_videos(\n",
    "    'TrainingData/videos',\n",
    "    'TrainingData/frames',\n",
    "    fps=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd9c49",
   "metadata": {},
   "source": [
    "# 4. Procesamiento de imágenes para FaceNet\n",
    "\n",
    "**IMPORTANTE:** Las imágenes se guardan en **RGB 160x160** (formato requerido por FaceNet)\n",
    "- RGB mantiene información de color (mejor que grayscale)\n",
    "- 160x160 es el tamaño de entrada estándar de FaceNet\n",
    "- Detección con Haar Cascade + validación múltiple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d885169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\Projects\\ProyectoFinalIA\\envPROJ\\Lib\\site-packages\\face_recognition_models\\__init__.py:7: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpeta de caras eliminada\n",
      "\n",
      "======================================================================\n",
      "PROCESANDO FRAMES Y EXTRAYENDO CARAS (RGB 160x160 para FaceNet)\n",
      "======================================================================\n",
      "\n",
      "Procesando: AlisonSalas\n",
      "  460 caras válidas extraídas | 0 rechazadas\n",
      "Procesando: FreddyAlvarez\n",
      "  460 caras válidas extraídas | 0 rechazadas\n",
      "Procesando: FreddyAlvarez\n",
      "  432 caras válidas extraídas | 0 rechazadas\n",
      "Procesando: IsmaelSailema\n",
      "  432 caras válidas extraídas | 0 rechazadas\n",
      "Procesando: IsmaelSailema\n",
      "  429 caras válidas extraídas | 0 rechazadas\n",
      "Procesando: WilliamChimborazo\n",
      "  429 caras válidas extraídas | 0 rechazadas\n",
      "Procesando: WilliamChimborazo\n",
      "  456 caras válidas extraídas | 0 rechazadas\n",
      "\n",
      "======================================================================\n",
      "RESUMEN DE PROCESAMIENTO:\n",
      "  Total de frames procesados: 2248\n",
      "  Total de caras extraídas: 1777\n",
      "  Total de detecciones rechazadas: 0\n",
      "  Tasa de precisión: 100.0%\n",
      "  Método utilizado: Haar Cascade + Validación múltiple\n",
      "  Validaciones: Proporción, Ojos, Varianza, Tamaño\n",
      "  Formato: RGB 160x160 (compatible con FaceNet)\n",
      "  Carpeta de caras: TrainingData\\faces\n",
      "  Formato de nombres: {persona}_face_{id}.png\n",
      "======================================================================\n",
      "  456 caras válidas extraídas | 0 rechazadas\n",
      "\n",
      "======================================================================\n",
      "RESUMEN DE PROCESAMIENTO:\n",
      "  Total de frames procesados: 2248\n",
      "  Total de caras extraídas: 1777\n",
      "  Total de detecciones rechazadas: 0\n",
      "  Tasa de precisión: 100.0%\n",
      "  Método utilizado: Haar Cascade + Validación múltiple\n",
      "  Validaciones: Proporción, Ojos, Varianza, Tamaño\n",
      "  Formato: RGB 160x160 (compatible con FaceNet)\n",
      "  Carpeta de caras: TrainingData\\faces\n",
      "  Formato de nombres: {persona}_face_{id}.png\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "# Rutas\n",
    "frames_base = Path('TrainingData/frames')\n",
    "faces_base = Path('TrainingData/faces')\n",
    "\n",
    "# Limpiar carpeta de caras si existe\n",
    "if faces_base.exists():\n",
    "    shutil.rmtree(faces_base)\n",
    "    print(f\"Carpeta de caras eliminada\\n\")\n",
    "\n",
    "faces_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# múltiples cascades para mejor detección\n",
    "cascade_frontal = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "cascade_profile = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_profileface.xml')\n",
    "cascade_eye = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "def is_valid_face(face_crop, top, right, bottom, left):\n",
    "    \"\"\"\n",
    "    Valida si la región detectada es realmente un rostro\n",
    "    Usa múltiples criterios: proporción, ojos, tamaño\n",
    "    \"\"\"\n",
    "    # 1. Verificar tamaño mínimo razonable\n",
    "    width = right - left\n",
    "    height = bottom - top\n",
    "    \n",
    "    if width < 40 or height < 40:\n",
    "        return False\n",
    "    \n",
    "    # 2. Verificar proporción (los rostros tienen proporción ~1:1.2)\n",
    "    aspect_ratio = width / height\n",
    "    if aspect_ratio < 0.6 or aspect_ratio > 1.5:\n",
    "        return False\n",
    "    \n",
    "    # 3. Detectar ojos dentro de la región del rostro (validación fuerte)\n",
    "    gray_face = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Buscar ojos en la mitad superior del rostro\n",
    "    upper_half = gray_face[0:int(height*0.6), :]\n",
    "    eyes = cascade_eye.detectMultiScale(\n",
    "        upper_half,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=3,\n",
    "        minSize=(10, 10)\n",
    "    )\n",
    "    \n",
    "    # Si no detecta al menos 1 ojo, probablemente no es un rostro\n",
    "    if len(eyes) < 1:\n",
    "        return False\n",
    "    \n",
    "    # 4. Verificar varianza de píxeles (evitar regiones muy uniformes)\n",
    "    variance = np.var(gray_face)\n",
    "    if variance < 100:  # Muy uniforme, probablemente no es un rostro\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def detect_faces_improved(image):\n",
    "    \"\"\"\n",
    "    Detecta caras usando Haar Cascade mejorado con validaciones múltiples\n",
    "    Retorna lista de ubicaciones validadas en formato (top, right, bottom, left)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Aplicar ecualización adaptativa de histograma para mejorar contraste\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        gray_enhanced = clahe.apply(gray)\n",
    "        \n",
    "        # Detectar rostros frontales con parámetros MÁS ESTRICTOS\n",
    "        faces_frontal = cascade_frontal.detectMultiScale(\n",
    "            gray_enhanced,\n",
    "            scaleFactor=1.05,      # Más preciso (antes 1.1)\n",
    "            minNeighbors=8,        # Más estricto (antes 5)\n",
    "            minSize=(60, 60),      # Tamaño mínimo mayor (antes 20x20)\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "        \n",
    "        # También detectar perfiles (opcional, pero útil)\n",
    "        faces_profile = cascade_profile.detectMultiScale(\n",
    "            gray_enhanced,\n",
    "            scaleFactor=1.05,\n",
    "            minNeighbors=8,\n",
    "            minSize=(60, 60)\n",
    "        )\n",
    "        \n",
    "        # Combinar detecciones\n",
    "        all_faces = []\n",
    "        \n",
    "        # Procesar rostros frontales\n",
    "        for (x, y, w, h) in faces_frontal:\n",
    "            all_faces.append((x, y, w, h))\n",
    "        \n",
    "        # Procesar perfiles\n",
    "        for (x, y, w, h) in faces_profile:\n",
    "            all_faces.append((x, y, w, h))\n",
    "        \n",
    "        # Eliminar duplicados (caras detectadas dos veces)\n",
    "        unique_faces = []\n",
    "        for face in all_faces:\n",
    "            x, y, w, h = face\n",
    "            is_duplicate = False\n",
    "            \n",
    "            for existing_face in unique_faces:\n",
    "                ex, ey, ew, eh = existing_face\n",
    "                # Calcular solapamiento\n",
    "                overlap_x = max(0, min(x + w, ex + ew) - max(x, ex))\n",
    "                overlap_y = max(0, min(y + h, ey + eh) - max(y, ey))\n",
    "                overlap_area = overlap_x * overlap_y\n",
    "                \n",
    "                if overlap_area > (w * h * 0.5):  # 50% de solapamiento\n",
    "                    is_duplicate = True\n",
    "                    break\n",
    "            \n",
    "            if not is_duplicate:\n",
    "                unique_faces.append(face)\n",
    "        \n",
    "        # Validar cada rostro detectado\n",
    "        validated_faces = []\n",
    "        for (x, y, w, h) in unique_faces:\n",
    "            # Convertir formato Haar (x,y,w,h) a (top,right,bottom,left)\n",
    "            top, right, bottom, left = y, x + w, y + h, x\n",
    "            \n",
    "            # Extraer región del rostro para validación\n",
    "            face_crop = image[top:bottom, left:right]\n",
    "            \n",
    "            # Validar si es realmente un rostro\n",
    "            if is_valid_face(face_crop, top, right, bottom, left):\n",
    "                validated_faces.append((top, right, bottom, left))\n",
    "        \n",
    "        return validated_faces\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en detección: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PROCESANDO FRAMES Y EXTRAYENDO CARAS (RGB 160x160 para FaceNet)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "total_frames = 0\n",
    "total_faces = 0\n",
    "total_rejected = 0\n",
    "\n",
    "# Iterar por carpetas de personas\n",
    "for person_folder in sorted(frames_base.iterdir()):\n",
    "    if not person_folder.is_dir():\n",
    "        continue\n",
    "    \n",
    "    person_name = person_folder.name\n",
    "    print(f\"Procesando: {person_name}\")\n",
    "    \n",
    "    # Crear carpeta para las caras de esta persona\n",
    "    person_faces_folder = faces_base / person_name\n",
    "    person_faces_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    face_count = 0\n",
    "    rejected_count = 0\n",
    "    frame_files = sorted(list(person_folder.glob('*.png')))\n",
    "    \n",
    "    for frame_file in frame_files:\n",
    "        total_frames += 1\n",
    "        \n",
    "        try:\n",
    "            # Cargar imagen en RGB (OpenCV carga en BGR, así que convertimos)\n",
    "            image = cv2.imread(str(frame_file))\n",
    "            \n",
    "            if image is None:\n",
    "                continue\n",
    "            \n",
    "            # Detectar caras con detección mejorada\n",
    "            face_locations = detect_faces_improved(image)\n",
    "            \n",
    "            # Procesar cada cara detectada y validada\n",
    "            for idx, (top, right, bottom, left) in enumerate(face_locations):\n",
    "                # Agregar padding (15 píxeles para mejor contexto)\n",
    "                padding = 15\n",
    "                top = max(0, top - padding)\n",
    "                left = max(0, left - padding)\n",
    "                bottom = min(image.shape[0], bottom + padding)\n",
    "                right = min(image.shape[1], right + padding)\n",
    "                \n",
    "                # Recortar la cara (mantener RGB)\n",
    "                face_crop = image[top:bottom, left:right]\n",
    "                \n",
    "                # Verificar que tiene tamaño mínimo adecuado\n",
    "                if face_crop.shape[0] < 60 or face_crop.shape[1] < 60:\n",
    "                    rejected_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Convertir de BGR (OpenCV) a RGB (FaceNet)\n",
    "                face_rgb = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Redimensionar a 160x160 (tamaño requerido por FaceNet)\n",
    "                face_resized = cv2.resize(face_rgb, (160, 160), interpolation=cv2.INTER_LANCZOS4)\n",
    "                \n",
    "                # Convertir de vuelta a BGR para guardar con OpenCV\n",
    "                face_bgr = cv2.cvtColor(face_resized, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                # Guardar cara con naming: {person_name}_face_{id}\n",
    "                output_filename = f'{person_name}_face_{face_count}.png'\n",
    "                output_path = person_faces_folder / output_filename\n",
    "                \n",
    "                cv2.imwrite(str(output_path), face_bgr)\n",
    "                face_count += 1\n",
    "                total_faces += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    total_rejected += rejected_count\n",
    "    print(f\"  {face_count} caras válidas extraídas | {rejected_count} rechazadas\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMEN DE PROCESAMIENTO:\")\n",
    "print(f\"  Total de frames procesados: {total_frames}\")\n",
    "print(f\"  Total de caras extraídas: {total_faces}\")\n",
    "print(f\"  Total de detecciones rechazadas: {total_rejected}\")\n",
    "print(f\"  Tasa de precisión: {(total_faces/(total_faces+total_rejected)*100):.1f}%\" if (total_faces+total_rejected) > 0 else \"N/A\")\n",
    "print(f\"  Método utilizado: Haar Cascade + Validación múltiple\")\n",
    "print(f\"  Validaciones: Proporción, Ojos, Varianza, Tamaño\")\n",
    "print(f\"  Formato: RGB 160x160 (compatible con FaceNet)\")\n",
    "print(f\"  Carpeta de caras: {faces_base}\")\n",
    "print(f\"  Formato de nombres: {{persona}}_face_{{id}}.png\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a11ea",
   "metadata": {},
   "source": [
    "# 5. Balanceo de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b256f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def balance_face_dataset(faces_folder, method='undersample', target_count=None, augment=True):\n",
    "    \"\"\"\n",
    "    Equilibra el dataset de rostros para que todas las personas tengan número similar de imágenes\n",
    "    \n",
    "    Args:\n",
    "        faces_folder: Carpeta con subcarpetas de rostros por persona\n",
    "        method: 'undersample' (reducir), 'oversample' (aumentar) o 'hybrid' (combinado)\n",
    "        target_count: Número objetivo de imágenes por persona (None = automático)\n",
    "        augment: Si True, usa data augmentation al hacer oversample\n",
    "    \n",
    "    Returns:\n",
    "        dict con estadísticas del balanceo\n",
    "    \"\"\"\n",
    "    faces_base = Path(faces_folder)\n",
    "    \n",
    "    # Contar imágenes por persona\n",
    "    person_counts = {}\n",
    "    for person_folder in sorted(faces_base.iterdir()):\n",
    "        if not person_folder.is_dir():\n",
    "            continue\n",
    "        \n",
    "        face_files = list(person_folder.glob('*.png'))\n",
    "        person_counts[person_folder.name] = {\n",
    "            'folder': person_folder,\n",
    "            'files': face_files,\n",
    "            'count': len(face_files)\n",
    "        }\n",
    "    \n",
    "    if not person_counts:\n",
    "        print(\"No se encontraron carpetas de personas\")\n",
    "        return None\n",
    "    \n",
    "    # Calcular estadísticas\n",
    "    counts = [p['count'] for p in person_counts.values()]\n",
    "    min_count = min(counts)\n",
    "    max_count = max(counts)\n",
    "    avg_count = sum(counts) // len(counts)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"ANÁLISIS DEL DATASET\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Personas: {len(person_counts)}\")\n",
    "    print(f\"Mínimo de imágenes: {min_count}\")\n",
    "    print(f\"Máximo de imágenes: {max_count}\")\n",
    "    print(f\"Promedio: {avg_count}\")\n",
    "    print(f\"Desbalance: {max_count - min_count} imágenes de diferencia\")\n",
    "    print(\"\\nDistribución por persona:\")\n",
    "    for name, data in sorted(person_counts.items(), key=lambda x: x[1]['count'], reverse=True):\n",
    "        print(f\"  {name:20s}: {data['count']:3d} imágenes \")\n",
    "    \n",
    "    # Determinar target_count según el método\n",
    "    if target_count is None:\n",
    "        if method == 'undersample':\n",
    "            target_count = min_count\n",
    "        elif method == 'oversample':\n",
    "            target_count = max_count\n",
    "        else:  # hybrid\n",
    "            target_count = avg_count\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"MÉTODO: {method.upper()} | OBJETIVO: {target_count} imágenes por persona\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    stats = {\n",
    "        'before': person_counts.copy(),\n",
    "        'after': {},\n",
    "        'removed': 0,\n",
    "        'created': 0\n",
    "    }\n",
    "    \n",
    "    # Aplicar balanceo\n",
    "    for person_name, data in person_counts.items():\n",
    "        current_count = data['count']\n",
    "        person_folder = data['folder']\n",
    "        files = data['files']\n",
    "        \n",
    "        if current_count == target_count:\n",
    "            print(f\"{person_name}: Ya tiene {target_count} imágenes\")\n",
    "            stats['after'][person_name] = current_count\n",
    "            continue\n",
    "        \n",
    "        if current_count > target_count:\n",
    "            # REDUCIR: Eliminar imágenes aleatorias\n",
    "            to_remove = current_count - target_count\n",
    "            files_to_remove = random.sample(files, to_remove)\n",
    "            \n",
    "            for file_path in files_to_remove:\n",
    "                file_path.unlink()\n",
    "                stats['removed'] += 1\n",
    "            \n",
    "            remaining = current_count - to_remove\n",
    "            stats['after'][person_name] = remaining\n",
    "            print(f\"{person_name}: Reducido de {current_count} → {remaining} (-{to_remove})\")\n",
    "        \n",
    "        else:\n",
    "            # AUMENTAR: Duplicar o aumentar con augmentation\n",
    "            to_add = target_count - current_count\n",
    "            \n",
    "            # Obtener el máximo número de archivo existente para continuar la numeración\n",
    "            existing_numbers = []\n",
    "            for f in files:\n",
    "                # Extraer número del nombre: {person_name}_face_{num}.png\n",
    "                stem = f.stem  # nombre sin extensión\n",
    "                parts = stem.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    try:\n",
    "                        num = int(parts[-1])\n",
    "                        existing_numbers.append(num)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            \n",
    "            # Comenzar desde el siguiente número disponible\n",
    "            next_face_id = max(existing_numbers) + 1 if existing_numbers else current_count\n",
    "            \n",
    "            if augment:\n",
    "                # Data Augmentation: rotaciones, flips, brillo (mantener RGB)\n",
    "                added = 0\n",
    "                while added < to_add:\n",
    "                    # Seleccionar imagen aleatoria para aumentar\n",
    "                    source_file = random.choice(files)\n",
    "                    # Cargar en color (RGB)\n",
    "                    img = cv2.imread(str(source_file), cv2.IMREAD_COLOR)\n",
    "                    \n",
    "                    if img is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Aplicar transformación aleatoria\n",
    "                    transform_type = random.choice(['rotate', 'flip', 'brightness', 'contrast'])\n",
    "                    \n",
    "                    if transform_type == 'rotate':\n",
    "                        # Rotar entre -15 y 15 grados\n",
    "                        angle = random.uniform(-15, 15)\n",
    "                        h, w = img.shape[:2]\n",
    "                        M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1.0)\n",
    "                        img_aug = cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "                    \n",
    "                    elif transform_type == 'flip':\n",
    "                        # Flip horizontal\n",
    "                        img_aug = cv2.flip(img, 1)\n",
    "                    \n",
    "                    elif transform_type == 'brightness':\n",
    "                        # Ajustar brillo\n",
    "                        beta = random.randint(-30, 30)\n",
    "                        img_aug = cv2.convertScaleAbs(img, alpha=1.0, beta=beta)\n",
    "                    \n",
    "                    else:  # contrast\n",
    "                        # Ajustar contraste\n",
    "                        alpha = random.uniform(0.8, 1.2)\n",
    "                        img_aug = cv2.convertScaleAbs(img, alpha=alpha, beta=0)\n",
    "                    \n",
    "                    # Guardar imagen aumentada con el mismo formato: {person_name}_face_{id}.png\n",
    "                    new_filename = f\"{person_name}_face_{next_face_id}.png\"\n",
    "                    new_path = person_folder / new_filename\n",
    "                    cv2.imwrite(str(new_path), img_aug)\n",
    "                    next_face_id += 1\n",
    "                    added += 1\n",
    "                    stats['created'] += 1\n",
    "                \n",
    "                final_count = current_count + added\n",
    "                stats['after'][person_name] = final_count\n",
    "                print(f\"{person_name}: Aumentado de {current_count} → {final_count} (+{added} con augmentation)\")\n",
    "            \n",
    "            else:\n",
    "                # Simple duplicación con el mismo formato\n",
    "                added = 0\n",
    "                while added < to_add:\n",
    "                    source_file = random.choice(files)\n",
    "                    new_filename = f\"{person_name}_face_{next_face_id}.png\"\n",
    "                    new_path = person_folder / new_filename\n",
    "                    shutil.copy2(source_file, new_path)\n",
    "                    next_face_id += 1\n",
    "                    added += 1\n",
    "                    stats['created'] += 1\n",
    "                \n",
    "                final_count = current_count + added\n",
    "                stats['after'][person_name] = final_count\n",
    "                print(f\"{person_name}: Aumentado de {current_count} → {final_count} (+{added} duplicados)\")\n",
    "    \n",
    "    # Resumen final\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RESUMEN DEL BALANCEO:\")\n",
    "    print(f\"  Imágenes eliminadas: {stats['removed']}\")\n",
    "    print(f\"  Imágenes creadas: {stats['created']}\")\n",
    "    print(f\"  Balance final: {target_count} imágenes por persona\")\n",
    "    print(f\"  Dataset balanceado: ✓ COMPLETO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa626157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANÁLISIS DEL DATASET\n",
      "======================================================================\n",
      "Personas: 4\n",
      "Mínimo de imágenes: 429\n",
      "Máximo de imágenes: 460\n",
      "Promedio: 444\n",
      "Desbalance: 31 imágenes de diferencia\n",
      "\n",
      "Distribución por persona:\n",
      "  AlisonSalas         : 460 imágenes \n",
      "  WilliamChimborazo   : 456 imágenes \n",
      "  FreddyAlvarez       : 432 imágenes \n",
      "  IsmaelSailema       : 429 imágenes \n",
      "\n",
      "======================================================================\n",
      "MÉTODO: HYBRID | OBJETIVO: 444 imágenes por persona\n",
      "======================================================================\n",
      "\n",
      "AlisonSalas: Reducido de 460 → 444 (-16)\n",
      "FreddyAlvarez: Aumentado de 432 → 444 (+12 con augmentation)\n",
      "IsmaelSailema: Aumentado de 429 → 444 (+15 con augmentation)\n",
      "WilliamChimborazo: Reducido de 456 → 444 (-12)\n",
      "\n",
      "======================================================================\n",
      "RESUMEN DEL BALANCEO:\n",
      "  Imágenes eliminadas: 28\n",
      "  Imágenes creadas: 27\n",
      "  Balance final: 444 imágenes por persona\n",
      "  Dataset balanceado: ✓ COMPLETO\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "balance_stats = balance_face_dataset(\n",
    "    faces_folder='TrainingData/faces',\n",
    "    method='hybrid',          \n",
    "    target_count=None,         # None = automático, o especifica un número como 100\n",
    "    augment=True               # True = usa data augmentation, False = duplica imágenes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d8217",
   "metadata": {},
   "source": [
    "# Verificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37cd58f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERIFICACIÓN FINAL DEL DATASET\n",
      "======================================================================\n",
      "\n",
      "Total de personas: 4\n",
      "Total de imágenes: 1776\n",
      "Promedio por persona: 444.0\n",
      "Rango: 444 - 444\n",
      "Desviación: 0\n",
      "\n",
      "Dataset PERFECTAMENTE equilibrado (diferencia ≤ 5)\n",
      "\n",
      "Distribución final:\n",
      "  AlisonSalas         : 444 imágenes\n",
      "  FreddyAlvarez       : 444 imágenes\n",
      "  IsmaelSailema       : 444 imágenes\n",
      "  WilliamChimborazo   : 444 imágenes\n",
      "\n",
      "======================================================================\n",
      "Dataset listo para entrenamiento del modelo\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Verificar el balance final del dataset\n",
    "from pathlib import Path\n",
    "\n",
    "faces_base = Path('TrainingData/faces')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VERIFICACIÓN FINAL DEL DATASET\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "person_counts = {}\n",
    "for person_folder in sorted(faces_base.iterdir()):\n",
    "    if not person_folder.is_dir():\n",
    "        continue\n",
    "    \n",
    "    face_count = len(list(person_folder.glob('*.png')))\n",
    "    person_counts[person_folder.name] = face_count\n",
    "\n",
    "if person_counts:\n",
    "    counts = list(person_counts.values())\n",
    "    min_count = min(counts)\n",
    "    max_count = max(counts)\n",
    "    avg_count = sum(counts) / len(counts)\n",
    "    \n",
    "    print(f\"Total de personas: {len(person_counts)}\")\n",
    "    print(f\"Total de imágenes: {sum(counts)}\")\n",
    "    print(f\"Promedio por persona: {avg_count:.1f}\")\n",
    "    print(f\"Rango: {min_count} - {max_count}\")\n",
    "    print(f\"Desviación: {max_count - min_count}\")\n",
    "    \n",
    "    if max_count - min_count <= 5:\n",
    "        print(f\"\\nDataset PERFECTAMENTE equilibrado (diferencia ≤ 5)\")\n",
    "    elif max_count - min_count <= 15:\n",
    "        print(f\"\\nDataset BIEN equilibrado (diferencia ≤ 15)\")\n",
    "    else:\n",
    "        print(f\"\\nDataset con desbalance (diferencia > 15)\")\n",
    "    \n",
    "    print(\"\\nDistribución final:\")\n",
    "    for name, count in sorted(person_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {name:20s}: {count:3d} imágenes\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Dataset listo para entrenamiento del modelo\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"No se encontraron carpetas de rostros\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPROJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
