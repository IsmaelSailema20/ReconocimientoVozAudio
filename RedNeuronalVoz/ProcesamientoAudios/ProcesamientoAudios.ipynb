{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5aebcf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c40bccd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio DIR: d:\\Desarrollo\\Universidad\\Septimo\\IA\\RedNeuronalVoz\\ProcesamientoAudios\\Audios\n",
      "Output DIR: d:\\Desarrollo\\Universidad\\Septimo\\IA\\RedNeuronalVoz\\ProcesamientoAudios\\ProcessedData\n"
     ]
    }
   ],
   "source": [
    "# ConfiguraciÃ³n\n",
    "AUDIO_DIR = Path(\"Audios\")\n",
    "OUTPUT_DIR = Path(\"ProcessedData\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 64\n",
    "N_MFCC = 13\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 160\n",
    "\n",
    "print(f\"Audio DIR: {AUDIO_DIR.absolute()}\")\n",
    "print(f\"Output DIR: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "157c2528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ AudioProcessor definido\n"
     ]
    }
   ],
   "source": [
    "class AudioProcessor:\n",
    "    \"\"\"Procesador de audios con PyTorch - ExtracciÃ³n robusta de caracterÃ­sticas\"\"\"\n",
    "    \n",
    "    def __init__(self, sr=SAMPLE_RATE, n_mels=N_MELS, n_mfcc=N_MFCC):\n",
    "        self.sr = sr\n",
    "        self.n_mels = n_mels\n",
    "        self.n_mfcc = n_mfcc\n",
    "    \n",
    "    def load_audio(self, filepath):\n",
    "        \"\"\"Carga y normaliza audio\"\"\"\n",
    "        try:\n",
    "            audio, sr = librosa.load(str(filepath), sr=self.sr, mono=True)\n",
    "            # Normalizar\n",
    "            audio = audio / (np.max(np.abs(audio)) + 1e-7)\n",
    "            return torch.FloatTensor(audio), sr\n",
    "        except Exception as e:\n",
    "            print(f\"Error cargando {filepath}: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def extract_mel_spectrogram(self, audio):\n",
    "        \"\"\"Extrae Mel-spectrogram (2D)\"\"\"\n",
    "        if isinstance(audio, torch.Tensor):\n",
    "            audio = audio.cpu().numpy()\n",
    "        \n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio, sr=self.sr, n_mels=self.n_mels,\n",
    "            n_fft=N_FFT, hop_length=HOP_LENGTH\n",
    "        )\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        # Normalizar por frecuencia\n",
    "        mel_spec = (mel_spec - mel_spec.mean(axis=1, keepdims=True)) / (mel_spec.std(axis=1, keepdims=True) + 1e-7)\n",
    "        return torch.FloatTensor(mel_spec)\n",
    "    \n",
    "    def extract_mfcc(self, audio):\n",
    "        \"\"\"Extrae MFCC + deltas (2D)\"\"\"\n",
    "        if isinstance(audio, torch.Tensor):\n",
    "            audio = audio.cpu().numpy()\n",
    "        \n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=self.sr, n_mfcc=self.n_mfcc)\n",
    "        delta = librosa.feature.delta(mfcc)\n",
    "        delta_delta = librosa.feature.delta(mfcc, order=2)\n",
    "        \n",
    "        features = np.vstack([mfcc, delta, delta_delta])\n",
    "        features = (features - features.mean(axis=1, keepdims=True)) / (features.std(axis=1, keepdims=True) + 1e-7)\n",
    "        return torch.FloatTensor(features)\n",
    "    \n",
    "    def extract_embedding(self, audio):\n",
    "        \"\"\"Extrae embedding: promedio Y desv. est. temporales + estadÃ­sticas\"\"\"\n",
    "        mel_spec = self.extract_mel_spectrogram(audio)  # (n_mels, time_steps)\n",
    "        mfcc = self.extract_mfcc(audio)  # (n_mfcc*3, time_steps)\n",
    "        \n",
    "        # ESTADÃSTICAS TEMPORALES de Mel-spectrogram\n",
    "        mel_mean = mel_spec.mean(dim=1)  # Promedio temporal (n_mels,)\n",
    "        mel_std = mel_spec.std(dim=1)    # Desv. estÃ¡ndar temporal (n_mels,)\n",
    "        mel_max = mel_spec.max(dim=1)[0] # MÃ¡ximo temporal (n_mels,)\n",
    "        \n",
    "        # ESTADÃSTICAS TEMPORALES de MFCC\n",
    "        mfcc_mean = mfcc.mean(dim=1)  # Promedio temporal\n",
    "        mfcc_std = mfcc.std(dim=1)    # Desv. estÃ¡ndar temporal\n",
    "        mfcc_max = mfcc.max(dim=1)[0] # MÃ¡ximo temporal\n",
    "        \n",
    "        # Combinar: media, desv.est, mÃ¡x para cada stream\n",
    "        # Total: (64+64+64) + (39+39+39) = 192 + 117 = 309 dims, pero reducimos...\n",
    "        embedding = torch.cat([\n",
    "            mel_mean, mel_std, mel_max,\n",
    "            mfcc_mean, mfcc_std, mfcc_max\n",
    "        ])  # (64*3 + 39*3,) = (309,)\n",
    "        \n",
    "        # Normalizar a [0,1] en lugar de L2 para preservar magnitud\n",
    "        embedding = (embedding - embedding.min()) / (embedding.max() - embedding.min() + 1e-7)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "print(\"âœ“ AudioProcessor definido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c66a654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ AudioAugmenter definido\n"
     ]
    }
   ],
   "source": [
    "class AudioAugmenter:\n",
    "    \"\"\"AugmentaciÃ³n de audios con PyTorch\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_noise(audio, noise_factor=0.005):\n",
    "        \"\"\"AÃ±ade ruido gaussiano\"\"\"\n",
    "        noise = torch.randn_like(audio) * noise_factor\n",
    "        return audio + noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def time_stretch(audio, rate=1.1):\n",
    "        \"\"\"Time stretching con librosa\"\"\"\n",
    "        audio_np = audio.cpu().numpy() if isinstance(audio, torch.Tensor) else audio\n",
    "        stretched = librosa.effects.time_stretch(audio_np, rate=rate)\n",
    "        return torch.FloatTensor(stretched)\n",
    "    \n",
    "    @staticmethod\n",
    "    def pitch_shift(audio, n_steps=2, sr=SAMPLE_RATE):\n",
    "        \"\"\"Pitch shifting con librosa\"\"\"\n",
    "        audio_np = audio.cpu().numpy() if isinstance(audio, torch.Tensor) else audio\n",
    "        shifted = librosa.effects.pitch_shift(audio_np, sr=sr, n_steps=n_steps)\n",
    "        return torch.FloatTensor(shifted)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_augmentations(audio, num_aug=12):\n",
    "        \"\"\"Genera EXACTAMENTE 12 augmentaciones distintas y variadas\"\"\"\n",
    "        augmentations = []\n",
    "        \n",
    "        # 1. Original\n",
    "        augmentations.append(audio)\n",
    "        \n",
    "        # 2-4. Ruido en 3 niveles\n",
    "        for noise_level in [0.003, 0.007, 0.012]:\n",
    "            augmentations.append(AudioAugmenter.add_noise(audio, noise_level))\n",
    "        \n",
    "        # 5-6. Time stretch\n",
    "        for rate in [0.92, 1.08]:\n",
    "            try:\n",
    "                augmentations.append(AudioAugmenter.time_stretch(audio, rate))\n",
    "            except:\n",
    "                augmentations.append(AudioAugmenter.add_noise(audio, 0.005))\n",
    "        \n",
    "        # 7-8. Pitch shift\n",
    "        for steps in [-2, 2]:\n",
    "            try:\n",
    "                augmentations.append(AudioAugmenter.pitch_shift(audio, n_steps=steps))\n",
    "            except:\n",
    "                augmentations.append(AudioAugmenter.add_noise(audio, 0.008))\n",
    "        \n",
    "        # 9. Ruido + time stretch\n",
    "        try:\n",
    "            noisy = AudioAugmenter.add_noise(audio, 0.005)\n",
    "            stretched = AudioAugmenter.time_stretch(noisy, rate=0.95)\n",
    "            augmentations.append(stretched)\n",
    "        except:\n",
    "            augmentations.append(AudioAugmenter.add_noise(audio, 0.010))\n",
    "        \n",
    "        # 10. Ruido + pitch shift\n",
    "        try:\n",
    "            noisy = AudioAugmenter.add_noise(audio, 0.008)\n",
    "            shifted = AudioAugmenter.pitch_shift(noisy, n_steps=1)\n",
    "            augmentations.append(shifted)\n",
    "        except:\n",
    "            augmentations.append(AudioAugmenter.add_noise(audio, 0.012))\n",
    "        \n",
    "        # 11. Time stretch + pitch shift\n",
    "        try:\n",
    "            stretched = AudioAugmenter.time_stretch(audio, rate=1.05)\n",
    "            shifted = AudioAugmenter.pitch_shift(stretched, n_steps=-1)\n",
    "            augmentations.append(shifted)\n",
    "        except:\n",
    "            augmentations.append(AudioAugmenter.add_noise(audio, 0.006))\n",
    "        \n",
    "        # 12. Ruido + time stretch + pitch shift\n",
    "        try:\n",
    "            noisy = AudioAugmenter.add_noise(audio, 0.010)\n",
    "            stretched = AudioAugmenter.time_stretch(noisy, rate=0.98)\n",
    "            shifted = AudioAugmenter.pitch_shift(stretched, n_steps=1)\n",
    "            augmentations.append(shifted)\n",
    "        except:\n",
    "            augmentations.append(AudioAugmenter.add_noise(audio, 0.015))\n",
    "        \n",
    "        return augmentations[:num_aug]\n",
    "\n",
    "print(\"âœ“ AudioAugmenter definido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7865bc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ AlisonSalas          - 3 audios\n",
      "ðŸ‘¤ FreddyAlvarez        - 2 audios\n",
      "ðŸ‘¤ IsmaelSailema        - 3 audios\n",
      "ðŸ‘¤ WilliamChimborazo    - 3 audios\n",
      "\n",
      "âœ“ Total: 4 usuarios, 11 archivos\n"
     ]
    }
   ],
   "source": [
    "# Explorar audios\n",
    "personas = {}\n",
    "\n",
    "if AUDIO_DIR.exists():\n",
    "    for persona_dir in sorted(AUDIO_DIR.iterdir()):\n",
    "        if persona_dir.is_dir():\n",
    "            nombre = persona_dir.name\n",
    "            archivos = list(persona_dir.glob(\"*.opus\")) + \\\n",
    "                       list(persona_dir.glob(\"*.wav\")) + \\\n",
    "                       list(persona_dir.glob(\"*.mp3\"))\n",
    "            if archivos:\n",
    "                personas[nombre] = archivos\n",
    "                print(f\"ðŸ‘¤ {nombre:20} - {len(archivos)} audios\")\n",
    "\n",
    "print(f\"\\nâœ“ Total: {len(personas)} usuarios, {sum(len(a) for a in personas.values())} archivos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d21a535e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Procesando audios...\n",
      "======================================================================\n",
      "\n",
      "ðŸ‘¤ AlisonSalas\n",
      "   âœ“ AlisonSalas3.opus (12 variantes)\n",
      "   âœ“ AlisonSalas4.opus (12 variantes)\n",
      "   âœ“ AlisonSalas5.opus (12 variantes)\n",
      "   â†’ 36 embeddings (shape: (36, 309))\n",
      "\n",
      "ðŸ‘¤ FreddyAlvarez\n",
      "   âœ“ FreddyAlvarez1.opus (12 variantes)\n",
      "   âœ“ FreddyAlvarez2.opus (12 variantes)\n",
      "   â†’ 24 embeddings (shape: (24, 309))\n",
      "\n",
      "ðŸ‘¤ IsmaelSailema\n",
      "   âœ“ IsmaelSailema.opus (12 variantes)\n",
      "   âœ“ IsmaelSailema2.opus (12 variantes)\n",
      "   âœ“ IsmaelSailema3.opus (12 variantes)\n",
      "   â†’ 36 embeddings (shape: (36, 309))\n",
      "\n",
      "ðŸ‘¤ WilliamChimborazo\n",
      "   âœ“ WilliamChimborazo1.opus (12 variantes)\n",
      "   âœ“ WilliamChimborazo2.opus (12 variantes)\n",
      "   âœ“ WilliamChimborazo3.opus (12 variantes)\n",
      "   â†’ 36 embeddings (shape: (36, 309))\n",
      "\n",
      "======================================================================\n",
      "âœ“ Procesamiento completado\n",
      "  AlisonSalas          - 36 embeddings (dim: 309)\n",
      "  FreddyAlvarez        - 24 embeddings (dim: 309)\n",
      "  IsmaelSailema        - 36 embeddings (dim: 309)\n",
      "  WilliamChimborazo    - 36 embeddings (dim: 309)\n"
     ]
    }
   ],
   "source": [
    "# Procesar audios\n",
    "processor = AudioProcessor(sr=SAMPLE_RATE, n_mels=N_MELS, n_mfcc=N_MFCC)\n",
    "augmenter = AudioAugmenter()\n",
    "\n",
    "embeddings_por_usuario = {}\n",
    "\n",
    "print(\"\\nðŸ”„ Procesando audios...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for nombre_usuario, archivos in personas.items():\n",
    "    embeddings_usuario = []\n",
    "    print(f\"\\nðŸ‘¤ {nombre_usuario}\")\n",
    "    \n",
    "    for archivo in archivos:\n",
    "        audio, sr = processor.load_audio(archivo)\n",
    "        \n",
    "        if audio is None:\n",
    "            continue\n",
    "        \n",
    "        # Generar augmentaciones (12x para mÃ¡s datos)\n",
    "        audios_variantes = augmenter.generate_augmentations(audio, num_aug=12)\n",
    "        \n",
    "        for audio_variant in audios_variantes:\n",
    "            try:\n",
    "                embedding = processor.extract_embedding(audio_variant)\n",
    "                embeddings_usuario.append(embedding.numpy())\n",
    "            except Exception as e:\n",
    "                print(f\"   Error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"   âœ“ {archivo.name} ({len(audios_variantes)} variantes)\")\n",
    "    \n",
    "    if embeddings_usuario:\n",
    "        embeddings_por_usuario[nombre_usuario] = np.array(embeddings_usuario)\n",
    "        print(f\"   â†’ {len(embeddings_usuario)} embeddings (shape: {embeddings_por_usuario[nombre_usuario].shape})\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"âœ“ Procesamiento completado\")\n",
    "for usuario, emb in embeddings_por_usuario.items():\n",
    "    print(f\"  {usuario:20} - {len(emb)} embeddings (dim: {emb.shape[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b808170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Embeddings guardados: ProcessedData\\Embeddings_PyTorch.pkl\n",
      "  TamaÃ±o: 159.8 KB\n"
     ]
    }
   ],
   "source": [
    "# Guardar embeddings\n",
    "pkl_file = OUTPUT_DIR / 'Embeddings_PyTorch.pkl'\n",
    "\n",
    "datos = {\n",
    "    'embeddings_por_usuario': embeddings_por_usuario,\n",
    "    'usuarios': list(embeddings_por_usuario.keys()),\n",
    "    'num_usuarios': len(embeddings_por_usuario),\n",
    "    'embedding_dimension': list(embeddings_por_usuario.values())[0].shape[1] if embeddings_por_usuario else 0,\n",
    "    'processor_config': {\n",
    "        'sample_rate': SAMPLE_RATE,\n",
    "        'n_mels': N_MELS,\n",
    "        'n_mfcc': N_MFCC\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(pkl_file, 'wb') as f:\n",
    "    pickle.dump(datos, f)\n",
    "\n",
    "print(f\"âœ“ Embeddings guardados: {pkl_file}\")\n",
    "print(f\"  TamaÃ±o: {pkl_file.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "afa76046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando audios de prueba...\n",
      "======================================================================\n",
      "  âœ“ Audio1I.opus -> Audio1I\n",
      "  âœ“ Audio2A.opus -> Audio2A\n",
      "  âœ“ Audio3R.opus -> Audio3R\n",
      "  âœ“ Audio4R.opus -> Audio4R\n",
      "  âœ“ Audio6W.opus -> Audio6W\n",
      "  âœ“ Audio7F.opus -> Audio7F\n",
      "\n",
      "âœ“ Test embeddings guardados: ProcessedData\\Embeddings_Prueba_PyTorch.pkl\n"
     ]
    }
   ],
   "source": [
    "# Procesar audios de prueba\n",
    "from pathlib import Path\n",
    "\n",
    "test_audio_dir = Path('../RedNeuronal/AudiosPrueba')\n",
    "test_output_dir = OUTPUT_DIR\n",
    "\n",
    "if test_audio_dir.exists():\n",
    "    print(\"\\nProcesando audios de prueba...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    test_embeddings = {}\n",
    "    \n",
    "    for item in sorted(test_audio_dir.iterdir()):\n",
    "        if item.is_file() and item.suffix.lower() in ['.opus', '.wav', '.mp3']:\n",
    "            usuario_name = item.stem\n",
    "            audio, sr = processor.load_audio(item)\n",
    "            \n",
    "            if audio is not None:\n",
    "                embedding = processor.extract_embedding(audio)\n",
    "                if usuario_name not in test_embeddings:\n",
    "                    test_embeddings[usuario_name] = []\n",
    "                test_embeddings[usuario_name].append(embedding.numpy())\n",
    "                print(f\"  âœ“ {item.name} -> {usuario_name}\")\n",
    "    \n",
    "    if test_embeddings:\n",
    "        test_embeddings = {k: np.array(v) for k, v in test_embeddings.items()}\n",
    "        \n",
    "        test_pkl = test_output_dir / 'Embeddings_Prueba_PyTorch.pkl'\n",
    "        test_data = {\n",
    "            'embeddings_por_usuario': test_embeddings,\n",
    "            'usuarios': list(test_embeddings.keys()),\n",
    "            'num_usuarios': len(test_embeddings),\n",
    "            'embedding_dimension': list(test_embeddings.values())[0].shape[1]\n",
    "        }\n",
    "        \n",
    "        with open(test_pkl, 'wb') as f:\n",
    "            pickle.dump(test_data, f)\n",
    "        \n",
    "        print(f\"\\nâœ“ Test embeddings guardados: {test_pkl}\")\n",
    "else:\n",
    "    print(f\"\\nNo encontrada: {test_audio_dir.absolute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
